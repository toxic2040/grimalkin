<p align="center">
  <img src="grimalkin.jpg" width="320" alt="Grimalkin">
</p>

<p align="center">
  <strong>Your private AI familiar â€” fully local, fully yours</strong>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/Ollama-000000?logo=ollama&logoColor=white" alt="Ollama">
  <img src="https://img.shields.io/badge/FAISS-000000?logo=faiss&logoColor=white" alt="FAISS">
  <img src="https://img.shields.io/badge/Gradio-FF8C00?logo=gradio&logoColor=white" alt="Gradio">
  <img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="MIT License">
</p>

---

**Single-file â€¢ 100% offline â€¢ Ollama + FAISS RAG**

Turn your personal documents into daily briefings, smart file organization, and instant Q&A â€” no cloud, no API keys, no tracking.

## âœ¨ Features

- **Daily Briefings** â€” Structured markdown reports (Summary â†’ Key Data â†’ Outlook) generated from your documents
- **Ask Anything** â€” RAG-powered Q&A over your entire local knowledge base
- **Smart File Sorting** â€” LLM classifies and auto-organizes your Downloads (RESEARCH / NOTES / MEETING / PERSONAL / misc)
- **Auto-Indexing** â€” Sorted files are automatically embedded into the search index
- **Background Watcher** â€” Monitors your Downloads folder and sorts new files in real time
- **Local Web UI** â€” Clean Gradio interface with tabs for every feature
- **Runs on Consumer Hardware** â€” AMD / NVIDIA / Apple Silicon â€” no GPU required (but it helps)

## ðŸš€ Quick Start (5 minutes)

### 1. Install Ollama
Download from [ollama.com](https://ollama.com), then pull the models:

```bash
ollama pull qwen3:8b             # main reasoning model
ollama pull nomic-embed-text     # embedding model